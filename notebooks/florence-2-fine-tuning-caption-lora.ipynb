{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47799ebb-c229-4b2e-a6fe-64704192672a",
   "metadata": {},
   "source": [
    "Kernel: huggingface1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e060117c-621d-4d09-a08b-8448b86ba64b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a1f824-9b7a-4b0f-ad5b-a881e1efcd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM  \n",
    "from PIL import Image\n",
    "import requests\n",
    "import copy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb34dcf3-d5e9-419d-bd4d-b965d28aa1fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import html\n",
    "import base64\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "# import supervision as sv\n",
    "\n",
    "from IPython.core.display import display, HTML #DeprecationWarning    \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    get_scheduler\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any, Tuple, Generator\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda56846-7c76-4d87-b3e3-a7d442a7af53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To import dataset, add relevant paths to system path\n",
    "\n",
    "dataset_path = '../dataset'\n",
    "util_path = '../util'\n",
    "\n",
    "for path_to_add in [dataset_path, util_path]:\n",
    "\n",
    "    # Select child directory\n",
    "    child_dir = os.path.abspath(path_to_add)  \n",
    "    # print(f'child_dir:{child_dir}')\n",
    "\n",
    "    # Add the child directory to sys.path  \n",
    "    if child_dir not in sys.path:  \n",
    "        sys.path.append(child_dir)\n",
    "        print(f'child_dir added to sys.path')\n",
    "    else:\n",
    "        print(f'child_dir already in sys.path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e1153e-2093-436a-9ab1-d0a3cf3e8d4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "from dataset import CaptionsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74520ec7-fb0e-4929-9da0-e7ac91a81209",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073dff67-5548-46d0-b056-3a59b833736d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# images_base_path = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/computeinstance10-gpu/code/datasets/face_mask/images'\n",
    "images_base_path = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/computeinstance10-1-gpu/code/datasets/face_mask/images'\n",
    "annotations_coco_path = '../annotations/face_bbox_annotations_sample.json' # File downloaded from AzureML as COCO file\n",
    "\n",
    "annotations_captions_jsonl_base_path = '../annotations' # Train and test files will be created at this path\n",
    "task = 'more_detailed_caption'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e25da9-8de3-4beb-9a2c-6dc8f68d1f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = 'microsoft/Florence-2-base' #<-- Testing\n",
    "\n",
    "# model_id = 'microsoft/Florence-2-large' # \n",
    "\n",
    "# model_id = 'microsoft/Florence-2-large-ft'\n",
    "\n",
    "# model_id = 'microsoft/Florence-2-base-ft' # <-- Tested\n",
    "revision = None #'refs/pr/6'\n",
    "\n",
    "print(f'model_id:{model_id}, revision:{revision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fccc3a3-6963-4e2b-9c59-8368a6566ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if model_id == 'microsoft/Florence-2-base-ft' or model_id == 'microsoft/Florence-2-base':\n",
    "    test_size = 0.1\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_WORKERS = 0\n",
    "    EPOCHS = 50 #200 #400 #200 #100\n",
    "    LR = 2e-6 #1e-6 #5e-6\n",
    "elif model_id == 'microsoft/Florence-2-large':\n",
    "    test_size = 0.1\n",
    "    BATCH_SIZE = 2 # Samller batch size for bigger model\n",
    "    NUM_WORKERS = 0\n",
    "    EPOCHS = 15 #200 #400 #200 #100\n",
    "    LR = 2e-6 #1e-6 #5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5722ea4-49d4-4bc2-a098-9e1bfd4b868a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device='cuda'\n",
    "\n",
    "print(f'device:{device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb53e371-1467-42f8-ac58-4a0a255783d4",
   "metadata": {},
   "source": [
    "Annotations created by 'generate-annotations.ipynb' notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3d19a-5a1a-4736-b326-894b2eb06f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_trimmed_files = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3cf09e-ef70-4b4a-9a68-8721b1edd899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if use_trimmed_files:\n",
    "    # Annotations created previously\n",
    "    annotations_jsonl_path_train = os.path.join(annotations_captions_jsonl_base_path, 'face_caption_annotations_sample' + '_train_trimmed' + '.jsonl')\n",
    "    annotations_jsonl_path_test = os.path.join(annotations_captions_jsonl_base_path, 'face_caption_annotations_sample' + '_test_trimmed' + '.jsonl')\n",
    "else:\n",
    "    # Annotations created previously\n",
    "    annotations_jsonl_path_train = os.path.join(annotations_captions_jsonl_base_path, 'face_caption_annotations_sample' + '_train' + '.jsonl')\n",
    "    annotations_jsonl_path_test = os.path.join(annotations_captions_jsonl_base_path, 'face_caption_annotations_sample' + '_test' + '.jsonl')\n",
    "    \n",
    "print(f'annotations_jsonl_path_train:{annotations_jsonl_path_train}')\n",
    "print(f'annotations_jsonl_path_test:{annotations_jsonl_path_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f70682-faec-4e24-bf5f-8bb395454e68",
   "metadata": {},
   "source": [
    "### Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3329e3c8-f4b9-456e-b2b2-9ae753a391f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_json(json_file_path):\n",
    "\n",
    "    json_data = None\n",
    "\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    return json_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e58c98-c598-4c07-86ca-4031874068c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example(model, task_prompt, text_input=None):   \n",
    "    \n",
    "    if text_input is None:\n",
    "        prompt = task_prompt\n",
    "    else:\n",
    "        prompt = task_prompt + text_input\n",
    "    \n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # MK\n",
    "    # Move the Input Data to GPU\n",
    "    if device == 'cuda':\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}  \n",
    "         \n",
    "    generated_ids = model.generate(\n",
    "      input_ids=inputs[\"input_ids\"],\n",
    "      pixel_values=inputs[\"pixel_values\"],\n",
    "      max_new_tokens=1024,\n",
    "      early_stopping=False,\n",
    "      do_sample=False,\n",
    "      num_beams=3,\n",
    "    )\n",
    "    \n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    \n",
    "    parsed_answer = processor.post_process_generation(\n",
    "        generated_text, \n",
    "        task=task_prompt, \n",
    "        image_size=(image.width, image.height)\n",
    "    )\n",
    "\n",
    "    return parsed_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4237b3fc-a375-44c5-b9ad-ce9314783817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(avg_train_loss_list, avg_val_loss_list, size=(10, 5), title='Training vs Validation Loss',   \n",
    "              x_label='Epochs', y_label='Loss', train_legend='Training Loss', val_legend='Validation Loss'):  \n",
    "    \n",
    "    # Set the size of the plot  \n",
    "    plt.figure(figsize=size)  \n",
    "      \n",
    "    # Plot training and validation loss  \n",
    "    plt.plot(avg_train_loss_list, label=train_legend)  \n",
    "    plt.plot(avg_val_loss_list, label=val_legend)  \n",
    "      \n",
    "    # Adding title and labels  \n",
    "    plt.title(title)  \n",
    "    plt.xlabel(x_label)  \n",
    "    plt.ylabel(y_label)  \n",
    "      \n",
    "    # Show legend  \n",
    "    plt.legend()  \n",
    "      \n",
    "    # Show the plot  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbbbfd-f0a6-40d9-8340-55734761cc44",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e136e15d-67c6-4916-a9b9-a7392f9cbd91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "processor = None\n",
    "\n",
    "if revision:\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).eval().to(device)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, revision = revision).to(device)\n",
    "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True, revision = revision)\n",
    "else: # Ignore revision\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(device)\n",
    "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)  \n",
    "\n",
    "print(f'Model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fdcbf0-b6e6-4d61-903b-221397ddcda2",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd57d74-ccc5-492e-bb24-5c901be5fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Dataset and DataLoader for train and validation subsets\n",
    "\n",
    "def collate_fn(batch):\n",
    "    questions, answers, images = zip(*batch)\n",
    "    # print(f'questions: {questions}, answers: {answers}, images:{images}')\n",
    "    inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True).to(device)\n",
    "    return inputs, answers\n",
    "\n",
    "train_dataset = CaptionsDataset(\n",
    "    jsonl_file_path = annotations_jsonl_path_train,\n",
    "    image_directory_path = images_base_path\n",
    ")\n",
    "\n",
    "val_dataset = CaptionsDataset(\n",
    "    jsonl_file_path = annotations_jsonl_path_test,\n",
    "    image_directory_path = images_base_path\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da29974-f230-4d2b-b6bd-71c949e07980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LoRA Florence-2 model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r= 32, #16, #8\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    "    use_rslora=True,\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    revision=revision\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86b0ae9-d66c-484e-840f-1da0834717cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca011f-aeb6-4567-b89e-2683c88959c7",
   "metadata": {},
   "source": [
    "### Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729215e-97c4-4055-9047-a5b16c20e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):\n",
    "    avg_train_loss_list = []\n",
    "    avg_val_loss_list = []\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    num_training_steps = epochs * len(train_loader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    # render_inference_results(peft_model, val_loader.dataset, 6)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, answers in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
    "            \n",
    "            # print(f'answers:{answers}')\n",
    "\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            pixel_values = inputs[\"pixel_values\"]\n",
    "            labels = processor.tokenizer(\n",
    "                text=answers,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                return_token_type_ids=False\n",
    "            ).input_ids.to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward(), optimizer.step(), lr_scheduler.step(), optimizer.zero_grad()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Average Training Loss: {avg_train_loss}\")\n",
    "        avg_train_loss_list.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, answers in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n",
    "\n",
    "                input_ids = inputs[\"input_ids\"]\n",
    "                pixel_values = inputs[\"pixel_values\"]\n",
    "                labels = processor.tokenizer(\n",
    "                    text=answers,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    return_token_type_ids=False\n",
    "                ).input_ids.to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            print(f\"Average Validation Loss: {avg_val_loss}\")\n",
    "            avg_val_loss_list.append(avg_val_loss)\n",
    "\n",
    "            # render_inference_results(peft_model, val_loader.dataset, 6)\n",
    "\n",
    "    # MK\n",
    "    # Save last epoch\n",
    "    checkpoint_path = '../model_checkpoints/' + model_id.replace('/','_').replace('-','_').replace(' ','_') + '_' + task + '_epoch_' + str(EPOCHS)\n",
    "    output_dir = checkpoint_path\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save_pretrained(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    \n",
    "    return avg_train_loss_list, avg_val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cbe678-8ddc-48ec-a88d-093176045bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "avg_train_loss_list, avg_val_loss_list = train_model(train_loader, val_loader, peft_model, processor, epochs=EPOCHS, lr=LR)\n",
    "\n",
    "plot_loss(avg_train_loss_list, avg_val_loss_list, size=(6, 3), title='Training vs Validation Loss',   \n",
    "              x_label='Epochs', y_label='Loss', train_legend='Training Loss', val_legend='Validation Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52242c92-c8f5-4eeb-b3cf-e37eeb484311",
   "metadata": {},
   "source": [
    "### Load fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194bc440-1aa1-49df-a3b5-17b03b3bec5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_path = '../model_checkpoints/' + model_id.replace('/','_').replace('-','_').replace(' ','_') + '_' + task + '_epoch_' + str(EPOCHS)\n",
    "\n",
    "model_ft = AutoModelForCausalLM.from_pretrained(checkpoint_path, trust_remote_code=True, revision = revision).to(device)\n",
    "processor = AutoProcessor.from_pretrained(checkpoint_path, trust_remote_code=True, revision = revision)\n",
    "\n",
    "print(f'Loaded checkpoint_path:{checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a13573-9934-4d15-bbc2-420b3a19b6f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_own_image = True\n",
    "image_path = os.path.join(images_base_path,'maksssksksss712.png')\n",
    "\n",
    "print(f'use_own_image: {use_own_image}')\n",
    "\n",
    "if not use_own_image:\n",
    "    url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "else:    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "# See input image\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b72deee-fa70-41dd-8cb3-3c608c9e2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# task_prompt = '<MORE_DETAILED_CAPTION_CUSTOM>'\n",
    "\n",
    "# print(f'use_own_image:{use_own_image}')\n",
    "\n",
    "# results = run_example(model_ft,task_prompt)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a61ba-256b-40a7-b52b-1c42e0c774c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "task_prompt = '<MORE_DETAILED_CAPTION>'\n",
    "\n",
    "print(f'use_own_image:{use_own_image}')\n",
    "\n",
    "results = run_example(model_ft,task_prompt)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a1d33-164a-4da5-830b-5e89388b9a88",
   "metadata": {},
   "source": [
    "DONE:\n",
    "- LORA fine-tune (base-ft model) - CAPTION + {} - 100 epochs, ~6 Hrs, microsoft_Florence_2_base_ft_more_detailed_caption_epoch_100\n",
    "- LORA fine-tune (large model) - CAPTION + {} - 50 epochs, ~8 Hrs, microsoft_Florence_2_large_more_detailed_caption_epoch_50\n",
    "- LORA fine-tune (base-ft model) - {} only - 25 epochs, ~42 min (V100), microsoft_Florence_2_base_ft_more_detailed_caption_epoch_25\n",
    "- LORA fine-tune (large model) - {} only - 15 epochs, ~60 min (V100), microsoft_Florence_2_large_more_detailed_caption_epoch_15\n",
    "\n",
    "- LORA fine-tune (base model) - {} only - 50 epochs, ~1 Hr 23 min (V100), microsoft_Florence_2_base_more_detailed_caption_epoch_50, LoRa r = 32\n",
    "\n",
    "TO DO (Use unique epoch count):\n",
    "\n",
    "- LORA fine-tune (base-ft model) - {} only - New task: '<MORE_DETAILED_CAPTION_CUSTOM>'\n",
    "- LORA fine-tune (large model) - {} only - New task: '<MORE_DETAILED_CAPTION_CUSTOM>'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (huggingface1)",
   "language": "python",
   "name": "huggingface1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
