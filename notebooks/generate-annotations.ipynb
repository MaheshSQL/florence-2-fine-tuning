{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dd39010-bdb9-479e-afb7-b6d3919a1874",
   "metadata": {},
   "source": [
    "Kernel: huggingface1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4509569-09ee-461d-b810-0ecf11b92a0c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9373cf8-9d14-48ee-8a34-39b2a0382346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48877aae-0347-4166-9d13-9c8eb4d57c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cece8f9-c3ce-41ec-b0b9-4fe835873317",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To import util, add relevant paths to system path\n",
    "util_path = '../util'\n",
    "  \n",
    "# Select child directory\n",
    "child_dir = os.path.abspath(util_path)  \n",
    "# print(f'child_dir:{child_dir}')\n",
    "  \n",
    "# Add the child directory to sys.path  \n",
    "if child_dir not in sys.path:  \n",
    "    sys.path.append(child_dir)\n",
    "    print(f'child_dir added to sys.path')\n",
    "else:\n",
    "    print(f'child_dir already in sys.path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc45c784-5ff2-4016-8fbb-f53a088a7935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from util import image_to_base64, getFileList, call_llm_api\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a2f86f-e747-411f-928e-ad19aa302fa9",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447ac24-3a4c-43af-8e53-95ca796df8c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images_base_path = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/computeinstance10-gpu/code/datasets/face_mask/images'\n",
    "annotations_captions_jsonl_base_path = '../annotations' # Train and test files will be created at this path (Task = <MORE_DETAILED_CAPTION>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb653a-c7c1-43a7-9a51-4e2c1b449659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GPT4V_ENDPOINT = 'https://XXXXX.openai.azure.com/openai/deployments/gpt-4o-global-standard/chat/completions'\n",
    "GPT4V_API_VERSION = '2024-05-01-preview' #'2024-02-15-preview'\n",
    "GPT4V_KEY = 'YOUR_API_KEY' #\"YOUR_API_KEY\"\n",
    "GPT4V_ENABLED = False # Must be set to True to generate annotations, safety switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bed7b7-e27e-48ee-a860-47fb31fc0e23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'GPT4V_ENABLED:{GPT4V_ENABLED}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79804ec8-3969-41be-8d1a-b06754a7f8c2",
   "metadata": {},
   "source": [
    "Specify which config to use, this will depending on the use case. Create addtional config (e.g. c) if required for a use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22a54b0-5fec-4e1d-9153-e3789cc32300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify which config to use\n",
    "\n",
    "# which_config_enabled = 'a' # Annotation caption as per few_shot_config_a few-shot examples\n",
    "which_config_enabled = 'b' # Annotation caption as per few_shot_config_b few-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6f9ea-43ee-4677-98a4-4015d3223f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom caption examples for LLM\n",
    "few_shot_config_a = {\n",
    "    \"maksssksksss0.png\":{\n",
    "        \"caption\": \"There are two people standing next to each other. There is a woman wearing a white jacket and a black hat. The woman on the left side is wearing a face mask on her face. There are lot of people in the background and appears to be a busy street.\",\n",
    "        \"everyone_wearing_mask\": \"no\",\n",
    "        \"anyone_wearing_glasses\": \"no\"\n",
    "    },\n",
    "    \"maksssksksss1.png\":{\n",
    "        \"caption\": \"There are people standing a que. A person at the front is checking the temperature with a digital thermo-meter. They are wearing warm cloths and some people are wearing masks. This is an indoor location.\",\n",
    "        \"everyone_wearing_mask\": \"no\",\n",
    "        \"anyone_wearing_glasses\": \"yes\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b233f4bb-3c5f-472b-bda9-cfcdec29d5dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is specific to config 'b' as shown in cell below\n",
    "no_of_questions = 4 # Expected from LLM, based on few-shot example format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b38ef-2fe4-48fb-bd35-14c526aca5cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom input-text-based annotation examples for LLM\n",
    "few_shot_config_b = {\n",
    "    \"maksssksksss0.png\":{\n",
    "        \"question_1\": \"What are people doing?\",\n",
    "        \"answer_1\": \"Walking, people are walking on a street.\",\n",
    "        \n",
    "        \"question_2\": \"Does this look like a photo taken indoors?\",\n",
    "        \"answer_2\": \"No, it looks like it was taken outdoors.\",\n",
    "        \n",
    "        \"question_3\": \"Is this photo taken during the day or night?\",\n",
    "        \"answer_3\": \"Evening, it appears to be taken during the evening (or night) due to the lighting.\",\n",
    "        \n",
    "        \"question_4\": \"Are people carrying any items?\",\n",
    "        \"answer_4\": \"Yes, one person is holding a mobile phone.\"\n",
    "    },\n",
    "    \"maksssksksss1.png\":{\n",
    "        \"question_1\": \"What are people doing?\",\n",
    "        \"answer_1\": \"Waiting, people are waiting in a queue in a crowded area for temperature check.\",\n",
    "        \n",
    "        \"question_2\": \"Does this look like a photo taken indoors?\",\n",
    "        \"answer_2\": \"Yes, it looks like it was taken indoors.\",\n",
    "        \n",
    "        \"question_3\": \"Is this photo taken during the day or night?\",\n",
    "        \"answer_3\": \"Unclear, as it is indoors and artificial lighting is used.\",\n",
    "        \n",
    "        \"question_4\": \"Are people carrying any items?\",\n",
    "        \"answer_4\": \"Yes, some people are carrying backpacks.\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa1fd9-b23b-4670-9f4b-3bda91e16b10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set which few-shot examples to be used\n",
    "\n",
    "if which_config_enabled =='a':\n",
    "    few_shot_config = few_shot_config_a\n",
    "    print(f'Config a')\n",
    "elif which_config_enabled =='b':\n",
    "    few_shot_config = few_shot_config_b\n",
    "    print(f'Config b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b71cf8e-65b8-4b04-acd7-144481531ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_example_count = 500 #10 # 50 # For how many images automated annotations to be created, these will be used for fine-tuning\n",
    "test_size = 0.1\n",
    "random_state = 42\n",
    "\n",
    "print(f'The code will create annotations for {training_example_count} images from the images_base_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce020b5-4966-4b52-b872-ae5d5c43c0c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Physical files that will be created (train and test)\n",
    "\n",
    "if which_config_enabled =='a':\n",
    "    annotations_jsonl_path_train = os.path.join(annotations_captions_jsonl_base_path, 'face_caption_annotations_sample' + '_train' + '.jsonl')\n",
    "    annotations_jsonl_path_test = os.path.join(annotations_captions_jsonl_base_path, 'face_caption_annotations_sample' + '_test' + '.jsonl')\n",
    "    print(f'Config a annotations')\n",
    "elif which_config_enabled =='b':\n",
    "    annotations_jsonl_path_train = os.path.join(annotations_captions_jsonl_base_path, 'face_input_text_annotations_sample' + '_train' + '.jsonl')\n",
    "    annotations_jsonl_path_test = os.path.join(annotations_captions_jsonl_base_path, 'face_input_text_annotations_sample' + '_test' + '.jsonl')\n",
    "    print(f'Config b annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a2965-38f4-41cb-9929-8f1c49e79afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"<MORE_DETAILED_CAPTION>\"\n",
    "# prefix = \"<MORE_DETAILED_CAPTION_CUSTOM>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fefd538-b5a5-4efc-b693-d782ea5b8616",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77475e9-135a-4378-b6b1-b5522462753d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create few-shot example, in format expected by LLM function\n",
    "encoded_image_text_pairs = [] # List of dictionaries\n",
    "\n",
    "for key in few_shot_config:\n",
    "    \n",
    "    few_shot_example = {}\n",
    "    \n",
    "    image_path = os.path.join(images_base_path, key)\n",
    "    # print(image_path)\n",
    "    # print(few_shot_config[key]) \n",
    "    \n",
    "    image_base64 = image_to_base64(image_path)\n",
    "    \n",
    "    few_shot_example[\"base64_image\"] = image_base64\n",
    "    few_shot_example[\"expected_response\"] = json.dumps(few_shot_config[key]) # Expected output text\n",
    "    \n",
    "    encoded_image_text_pairs.append(few_shot_example)\n",
    "    \n",
    "# print(f'encoded_image_text_pairs:{encoded_image_text_pairs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a605f0ab-63cb-4d5c-91d0-b834cea9b140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all image files\n",
    "image_list = getFileList(images_base_path, '.png')\n",
    "# print(image_list)\n",
    "\n",
    "# Take subset based on config value\n",
    "image_list = image_list[0:training_example_count]\n",
    "# print(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63168869-652e-4d9d-a804-aaa34a4d874f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(len(getFileList(images_base_path, '.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b51588f-6d04-4c57-90fc-3033ae5a6ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Config 'a'\n",
    "if GPT4V_ENABLED and which_config_enabled =='a':\n",
    "    \n",
    "    print(f'Processing {len(image_list)} images, which_config_enabled {which_config_enabled}')\n",
    "\n",
    "    image_annotations = []\n",
    "\n",
    "    for img in tqdm(image_list):\n",
    "\n",
    "        img_name = os.path.basename(img)\n",
    "\n",
    "        # Generate for images not in few-shot example list\n",
    "        if img_name not in few_shot_config.keys():\n",
    "            # print(img)\n",
    "\n",
    "            encoded_image = image_to_base64(img) \n",
    "            llm_response_json = call_llm_api(GPT4V_ENDPOINT, GPT4V_API_VERSION, GPT4V_KEY, encoded_image, encoded_image_text_pairs, which_config_enabled)\n",
    "            # print(f'llm_response_json:{json.dumps(llm_response_json, indent=4)}')\n",
    "\n",
    "            # Check if message available as part of the response\n",
    "            if llm_response_json and \"choices\" in llm_response_json.keys() and len(llm_response_json[\"choices\"]) > 0:\n",
    "                # print(llm_response_json[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "                jsonl_response = {\"image\":img_name, \"prefix\":prefix, \"suffix\":llm_response_json[\"choices\"][0][\"message\"][\"content\"]}\n",
    "                # print(f'jsonl_response:{jsonl_response}')\n",
    "\n",
    "                image_annotations.append(jsonl_response)\n",
    "\n",
    "    print(f'Completed, which_config_enabled: {which_config_enabled}')\n",
    "    \n",
    "    \n",
    "# Config 'b'\n",
    "if GPT4V_ENABLED and which_config_enabled =='b':\n",
    "    \n",
    "    print(f'Processing {len(image_list)} images, which_config_enabled {which_config_enabled}')\n",
    "\n",
    "    image_annotations = []\n",
    "\n",
    "    for img in tqdm(image_list):\n",
    "\n",
    "        img_name = os.path.basename(img)\n",
    "\n",
    "        # Generate for images not in few-shot example list\n",
    "        if img_name not in few_shot_config.keys():\n",
    "            # print(img)\n",
    "\n",
    "            encoded_image = image_to_base64(img) \n",
    "            llm_response_json = call_llm_api(GPT4V_ENDPOINT, GPT4V_API_VERSION, GPT4V_KEY, encoded_image, encoded_image_text_pairs, which_config_enabled)\n",
    "            # print(f'llm_response_json:{json.dumps(llm_response_json, indent=4)}')\n",
    "\n",
    "            # Check if message available as part of the response\n",
    "            if llm_response_json and \"choices\" in llm_response_json.keys() and len(llm_response_json[\"choices\"]) > 0:\n",
    "                # print(llm_response_json[\"choices\"][0][\"message\"][\"content\"])               \n",
    "                \n",
    "                jsonl_response = {\"image\":img_name, \"content\": json.loads(llm_response_json[\"choices\"][0][\"message\"][\"content\"])}\n",
    "                # print(f'jsonl_response:{jsonl_response}')\n",
    "\n",
    "                image_annotations.append(jsonl_response)\n",
    "\n",
    "    print(f'Completed, which_config_enabled: {which_config_enabled}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f332ed2e-06af-430f-a324-075d057b26ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38fe378-05b5-4a36-b274-ff1ed5d4b326",
   "metadata": {},
   "source": [
    "Materialise the responses into JSON file records. This is where you decide how the response is transfromed and saved into annotation of exptected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7c6cb9-274c-480d-b3e9-10707cd75f12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Config 'a'\n",
    "if GPT4V_ENABLED and which_config_enabled =='a':\n",
    "    \n",
    "    # Split the list into train and test sets\n",
    "    image_annotations_train, image_annotations_test = train_test_split(image_annotations, test_size=test_size, random_state=random_state)  \n",
    "    print(f'len(image_annotations_train):{len(image_annotations_train)}')\n",
    "    print(f'len(image_annotations_test):{len(image_annotations_test)}')\n",
    "\n",
    "    # Save to line_dict_list into a jsonl file (train)\n",
    "    with open(annotations_jsonl_path_train, 'w') as file1:  \n",
    "        for dictionary1 in image_annotations_train:  \n",
    "            # Convert the dictionary to a JSON string  \n",
    "            json_str = json.dumps(dictionary1)  \n",
    "            # Write the JSON string to the file followed by a newline  \n",
    "            file1.write(json_str + '\\n')  \n",
    "\n",
    "    # Save to line_dict_list into a jsonl file (test)\n",
    "    with open(annotations_jsonl_path_test, 'w') as file2:  \n",
    "        for dictionary2 in image_annotations_test:  \n",
    "            # Convert the dictionary to a JSON string  \n",
    "            json_str = json.dumps(dictionary2)  \n",
    "            # Write the JSON string to the file followed by a newline  \n",
    "            file2.write(json_str + '\\n') \n",
    "\n",
    "    print(f'Files created: \\n{annotations_jsonl_path_train}, \\n{annotations_jsonl_path_test}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f12f14a-ac08-4f26-a826-908f7720b87e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Config 'b'\n",
    "if GPT4V_ENABLED and which_config_enabled =='b':   \n",
    "    \n",
    "    # Split the list into train and test sets\n",
    "    image_annotations_train, image_annotations_test = train_test_split(image_annotations, test_size=test_size, random_state=random_state)  \n",
    "    print(f'len(image_annotations_train):{len(image_annotations_train)}')\n",
    "    print(f'len(image_annotations_test):{len(image_annotations_test)}')\n",
    "    \n",
    "    # Save to line_dict_list into a jsonl file (train)\n",
    "    with open(annotations_jsonl_path_train, 'w') as file1:  \n",
    "        for dictionary1 in image_annotations_train: \n",
    "            # print(f'dictionary1:{dictionary1}')\n",
    "            # print(dictionary1.keys())\n",
    "            \n",
    "            # Split response into individual question/answer pairs\n",
    "            for question_no in range(no_of_questions):\n",
    "                # print(question_no)\n",
    "            \n",
    "                # Check if question/answer pair exists\n",
    "                if f'question_{question_no+1}' in dictionary1[\"content\"].keys() and f'answer_{question_no+1}' in dictionary1[\"content\"].keys():\n",
    "                    qa_pair_response = {\"image\":dictionary1[\"image\"], \n",
    "                                      \"prefix\":dictionary1[\"content\"][f'question_{question_no+1}'],                                       \n",
    "                                      \"suffix\":dictionary1[\"content\"][f'answer_{question_no+1}']\n",
    "                                     }\n",
    "                    # print(f'qa_pair_response:{qa_pair_response}')\n",
    "                    \n",
    "                    # Convert the dictionary to a JSON string  \n",
    "                    json_str = json.dumps(qa_pair_response)  \n",
    "                    # Write the JSON string to the file followed by a newline  \n",
    "                    file1.write(json_str + '\\n')  \n",
    "                    \n",
    "    # Save to line_dict_list into a jsonl file (test)\n",
    "    with open(annotations_jsonl_path_test, 'w') as file2:  \n",
    "        for dictionary2 in image_annotations_test: \n",
    "            # print(f'dictionary2:{dictionary2}')\n",
    "            # print(dictionary2.keys())\n",
    "            \n",
    "            # Split response into individual question/answer pairs\n",
    "            for question_no in range(no_of_questions):\n",
    "                # print(question_no)\n",
    "            \n",
    "                # Check if question/answer pair exists\n",
    "                if f'question_{question_no+1}' in dictionary2[\"content\"].keys() and f'answer_{question_no+1}' in dictionary2[\"content\"].keys():\n",
    "                    qa_pair_response = {\"image\":dictionary2[\"image\"], \n",
    "                                      \"prefix\":dictionary2[\"content\"][f'question_{question_no+1}'],                                       \n",
    "                                      \"suffix\":dictionary2[\"content\"][f'answer_{question_no+1}']\n",
    "                                     }\n",
    "                    # print(f'qa_pair_response:{qa_pair_response}')\n",
    "                    \n",
    "                    # Convert the dictionary to a JSON string  \n",
    "                    json_str = json.dumps(qa_pair_response)  \n",
    "                    # Write the JSON string to the file followed by a newline  \n",
    "                    file2.write(json_str + '\\n')\n",
    "                    \n",
    "    print(f'Files created: \\n{annotations_jsonl_path_train}, \\n{annotations_jsonl_path_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b72a96-3bb8-4d67-a159-aa80ab6f548d",
   "metadata": {},
   "source": [
    "### Create annotation copy without Caption\n",
    "These new files with _trimmed suffix will not full have caption, but only have dictionary format\n",
    "\n",
    "e.g.\n",
    "{\"image\": \"maksssksksss539.png\", \"prefix\": \"<MORE_DETAILED_CAPTION>\", \"suffix\": \"{\\\"everyone_wearing_mask\\\": \\\"no\\\", \\\"anyone_wearing_glasses\\\": \\\"yes\\\"}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969e466-cf46-4bdb-a305-068f1fcffc09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotations_jsonl_path_train_trimmed = annotations_jsonl_path_train.replace('_train.jsonl','_train_trimmed.jsonl')\n",
    "annotations_jsonl_path_test_trimmed = annotations_jsonl_path_test.replace('_test.jsonl','_test_trimmed.jsonl')\n",
    "\n",
    "print(f'annotations_jsonl_path_train_trimmed:{annotations_jsonl_path_train_trimmed}')\n",
    "print(f'annotations_jsonl_path_test_trimmed:{annotations_jsonl_path_test_trimmed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44acf37c-d0b2-45f6-af51-db74fb7985c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Config 'a'\n",
    "if which_config_enabled =='a':\n",
    "    \n",
    "    # Open the JSONL file and write to a new file - train\n",
    "    with open(annotations_jsonl_path_train, 'r') as file, open(annotations_jsonl_path_train_trimmed, 'w') as file_w:\n",
    "        for line in file:          \n",
    "\n",
    "            # the JSON object from the current line  \n",
    "            json_obj = json.loads(line)            \n",
    "            # print(json_obj)        \n",
    "\n",
    "            json_obj_trimmed = {\"image\":json_obj[\"image\"],\n",
    "                                \"prefix\":json_obj[\"prefix\"],\n",
    "                                \"suffix\":{ \"everyone_wearing_mask\" : json.loads(json_obj[\"suffix\"])[\"everyone_wearing_mask\"],\n",
    "                                           \"anyone_wearing_glasses\" : json.loads(json_obj[\"suffix\"])[\"anyone_wearing_glasses\"]\n",
    "                                         }\n",
    "                               }\n",
    "\n",
    "             # Convert the dictionary to a JSON string  \n",
    "            json_str = json.dumps(json_obj_trimmed)  \n",
    "            # Write the JSON string to the file followed by a newline  \n",
    "            file_w.write(json_str + '\\n') \n",
    "\n",
    "\n",
    "    # Open the JSONL file and write to a new file - test\n",
    "    with open(annotations_jsonl_path_test, 'r') as file, open(annotations_jsonl_path_test_trimmed, 'w') as file_w:\n",
    "        for line in file:          \n",
    "\n",
    "            # the JSON object from the current line  \n",
    "            json_obj = json.loads(line)            \n",
    "            # print(json_obj)        \n",
    "\n",
    "            json_obj_trimmed = {\"image\":json_obj[\"image\"],\n",
    "                                \"prefix\":json_obj[\"prefix\"],\n",
    "                                \"suffix\":{ \"everyone_wearing_mask\" : json.loads(json_obj[\"suffix\"])[\"everyone_wearing_mask\"],\n",
    "                                           \"anyone_wearing_glasses\" : json.loads(json_obj[\"suffix\"])[\"anyone_wearing_glasses\"]\n",
    "                                         }\n",
    "                               }\n",
    "\n",
    "             # Convert the dictionary to a JSON string  \n",
    "            json_str = json.dumps(json_obj_trimmed)  \n",
    "            # Write the JSON string to the file followed by a newline  \n",
    "            file_w.write(json_str + '\\n') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (huggingface1)",
   "language": "python",
   "name": "huggingface1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
