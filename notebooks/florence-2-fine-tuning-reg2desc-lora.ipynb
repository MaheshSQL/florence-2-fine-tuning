{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96741525-0df1-466a-905c-58300735516e",
   "metadata": {},
   "source": [
    "Kernel: huggingface1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dc6018-bc7d-409a-97ca-7f8d1f9d330e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04915b5-7dbc-427a-9356-a18021184f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM  \n",
    "from PIL import Image\n",
    "import requests\n",
    "import copy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c490fb58-5149-4b64-9175-f88c314d484a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import html\n",
    "import base64\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "# import supervision as sv\n",
    "\n",
    "from IPython.core.display import display, HTML #DeprecationWarning    \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    get_scheduler\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any, Tuple, Generator\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058888f-efab-440e-9cf1-3e03bf6c5e79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To import dataset, add relevant paths to system path\n",
    "\n",
    "dataset_path = '../dataset'\n",
    "  \n",
    "# Select child directory\n",
    "child_dir = os.path.abspath(dataset_path)  \n",
    "# print(f'child_dir:{child_dir}')\n",
    "  \n",
    "# Add the child directory to sys.path  \n",
    "if child_dir not in sys.path:  \n",
    "    sys.path.append(child_dir)\n",
    "    print(f'child_dir added to sys.path')\n",
    "else:\n",
    "    print(f'child_dir already in sys.path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4964187e-976c-4c45-a7cc-5e3dce503366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "from dataset import Region2DescDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf33152-be7a-424b-b2ee-2345169d918c",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee082efb-c6b5-470c-b442-39b3a50dcbd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images_base_path = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/computeinstance10-gpu/code/datasets/face_mask/images'\n",
    "annotations_coco_path = '../annotations/face_bbox_annotations_sample.json' # File downloaded from AzureML as COCO file\n",
    "\n",
    "annotations_jsonl_base_path = '../annotations' # Train and test files will be created at this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19542b48-4b0a-49c7-a1cc-9f9e8ab87fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_id = 'microsoft/Florence-2-base'\n",
    "\n",
    "model_id = 'microsoft/Florence-2-large' # <-- Testing\n",
    "\n",
    "# model_id = 'microsoft/Florence-2-large-ft'\n",
    "\n",
    "# model_id = 'microsoft/Florence-2-base-ft' # <-- Tested\n",
    "revision = None #'refs/pr/6'\n",
    "\n",
    "print(f'model_id:{model_id}, revision:{revision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa93be9-a0ae-43ca-8f29-644b49b44416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if model_id == 'microsoft/Florence-2-base-ft':\n",
    "    test_size = 0.1\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_WORKERS = 0\n",
    "    EPOCHS = 201 #200 #400 #200 #100\n",
    "    LR = 2e-6 #1e-6 #5e-6\n",
    "elif model_id == 'microsoft/Florence-2-large':\n",
    "    test_size = 0.1\n",
    "    BATCH_SIZE = 2 # Samller batch size for bigger model\n",
    "    NUM_WORKERS = 0\n",
    "    EPOCHS = 50 #200 #400 #200 #100\n",
    "    LR = 4e-6 #1e-6 #5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c13f5-328c-41fd-8877-c473cf676356",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device='cuda'\n",
    "\n",
    "print(f'device:{device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d52dc4-f59e-4251-a54b-260399c0b6d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Physical files that will be created (train and test)\n",
    "annotations_jsonl_path_train = os.path.join(annotations_jsonl_base_path, 'face_bbox_annotations_sample' + '_train' + '.jsonl')\n",
    "annotations_jsonl_path_test = os.path.join(annotations_jsonl_base_path, 'face_bbox_annotations_sample' + '_test' + '.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3423f7-e075-4f64-a0e1-25de5fd38a32",
   "metadata": {},
   "source": [
    "### Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67a2df-8a34-4a13-a7bd-e3f1e7412dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_json(json_file_path):\n",
    "\n",
    "    json_data = None\n",
    "\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    return json_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e8d22f-efc9-4b8f-b3c0-0e5c2917e992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lookup_category(category_id, categories_list):\n",
    "    \n",
    "    category_name = None\n",
    "    \n",
    "    for item in categories_list:\n",
    "        \n",
    "        if item[\"id\"] == category_id:\n",
    "            category_name = item[\"name\"]\n",
    "            break\n",
    "    \n",
    "    return category_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfdf598-70d7-42b3-8f1f-adb2bd4477ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_bbox(single_bbox):\n",
    "    \n",
    "    formatted_coordinates = ['<loc_{}>'.format(coord) for coord in single_bbox]      \n",
    "    \n",
    "    # Join the formatted strings into a single string without any spaces  \n",
    "    formatted_coordinates_str = ''.join(formatted_coordinates)  \n",
    "    \n",
    "    return formatted_coordinates_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3f792-32b2-44c1-a642-d7e326758f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task specific conversion\n",
    "def coco_to_jsonl_reg2desc(annotations_coco_path, annotations_jsonl_path_train, annotations_jsonl_path_test, test_size, random_state=42):\n",
    "    \n",
    "    # To store final output jsonl. each jsonl is a dict.\n",
    "    line_dict_list = []\n",
    "    \n",
    "    # Read json\n",
    "    annotations_json = load_json(annotations_coco_path)\n",
    "    # print(f'annotations_json:{json.dumps(annotations_json, indent=4)}')\n",
    "    \n",
    "    # For each image\n",
    "    for image in annotations_json[\"images\"]:\n",
    "        bbox_list = []\n",
    "        line_dict = {}\n",
    "        suffix_str = ''\n",
    "        prefix_str = '<REGION_TO_DESCRIPTION>'\n",
    "        \n",
    "        # print(f'image[\"id\"]:{image[\"id\"]}')\n",
    "        # print(f'image[\"file_name\"]:{image[\"file_name\"]}')         \n",
    "        \n",
    "        # All annotations in same file\n",
    "        for annotation in annotations_json[\"annotations\"]:\n",
    "            # print(f'annotation:{annotation}')\n",
    "\n",
    "            # All annotations for a particular image in same file\n",
    "            if image[\"id\"] == annotation[\"image_id\"]: \n",
    "                \n",
    "                ###########\n",
    "                \n",
    "                # Normalized COCO coordinates  \n",
    "                x_min_norm, y_min_norm, width_norm, height_norm = annotation[\"bbox\"] \n",
    "                \n",
    "                # Convert normalized coordinates to absolute coordinates on a scale of 1000  \n",
    "                x_min_abs = x_min_norm * 1000  \n",
    "                y_min_abs = y_min_norm * 1000  \n",
    "                width_abs = width_norm * 1000  \n",
    "                height_abs = height_norm * 1000  \n",
    "                \n",
    "                # Calculate x_max and y_max  \n",
    "                x_max_abs = x_min_abs + width_abs  \n",
    "                y_max_abs = y_min_abs + height_abs \n",
    "\n",
    "                # # From COCO normalized to x1,y1,x2,y2                \n",
    "                # img_width = image[\"width\"]\n",
    "                # img_height = image[\"height\"]\n",
    "\n",
    "                # Absolute coordinates on a scale of 1000 (x1, y1, x2, y2)  \n",
    "                x1 = int(x_min_abs)\n",
    "                y1 = int(y_min_abs)\n",
    "                x2 = int(x_max_abs)\n",
    "                y2 = int(y_max_abs)\n",
    "                \n",
    "                bbox_x1y1x2y2 = [x1,y1,x2,y2]\n",
    "                \n",
    "                ###########\n",
    "                \n",
    "                # This is the expected format for '<REGION_TO_DESCRIPTION>' task\n",
    "                # bbox_converted = [round(value * 1000) for value in annotation[\"bbox\"]]\n",
    "                # bbox_converted = [round(value * 1000) for value in bbox_x1y1x2y2]\n",
    "                bbox_converted = bbox_x1y1x2y2\n",
    "                # print(f'bbox_converted:{bbox_converted}')\n",
    "                \n",
    "                bbox_formatted = format_bbox(bbox_converted)\n",
    "                # print(f'bbox_formatted:{bbox_formatted}')\n",
    "                bbox_list.append(bbox_formatted)                 \n",
    "                \n",
    "                annotation_category = lookup_category(annotation[\"category_id\"], annotations_json[\"categories\"])\n",
    "                # print(f'annotation_category:{annotation_category}')\n",
    "                \n",
    "                # In labeling project: Re-label with 'mask' and 'no-mask' - DONE\n",
    "                # Then create suffix_str based on label name. e.g. 9 of clubs<><><><>10 of clubs<><><><>\n",
    "                \n",
    "                # Create suffix string e.g. \"9 of clubs<loc_138><loc_100><loc_470><loc_448>10 of clubs<loc_388><loc_145><loc_670><loc_453>\"\n",
    "                if annotation_category == 'No_Mask':\n",
    "                    suffix_str += 'Not wearing a mask' + bbox_formatted\n",
    "                elif annotation_category == 'Mask':\n",
    "                    suffix_str += 'Wearing a mask' + bbox_formatted\n",
    "                \n",
    "                \n",
    "        # print(f'suffix_str:{suffix_str}')\n",
    "        # print(f'bbox_list:{bbox_list}') \n",
    "        \n",
    "        # Update dictionary with image, prefix, suffix\n",
    "        line_dict[\"image\"] = image[\"file_name\"].split('/')[1] # Keepm only image name with extension\n",
    "        line_dict[\"prefix\"] = prefix_str\n",
    "        line_dict[\"suffix\"] = suffix_str        \n",
    "        \n",
    "        # Add line_dict to list\n",
    "        line_dict_list.append(line_dict)              \n",
    "                       \n",
    "        \n",
    "    # print(f'line_dict_list:{line_dict_list}')\n",
    "    \n",
    "    # Split the list into train and test sets\n",
    "    line_dict_list_train, line_dict_list_test = train_test_split(line_dict_list, test_size=test_size, random_state=random_state)  \n",
    "    \n",
    "    # print(f'line_dict_list_train:{line_dict_list_train}')\n",
    "    # print()\n",
    "    # print(f'line_dict_list_test:{line_dict_list_test}')   \n",
    "    \n",
    "    # Delete the jsonl output file if it exists\n",
    "    if os.path.exists(annotations_jsonl_path_train):  \n",
    "        os.remove(annotations_jsonl_path_train)      \n",
    "    if os.path.exists(annotations_jsonl_path_test):  \n",
    "        os.remove(annotations_jsonl_path_test)      \n",
    "    \n",
    "    # Save to line_dict_list into a jsonl file (train)\n",
    "    with open(annotations_jsonl_path_train, 'w') as file1:  \n",
    "        for dictionary1 in line_dict_list_train:  \n",
    "            # Convert the dictionary to a JSON string  \n",
    "            json_str = json.dumps(dictionary1)  \n",
    "            # Write the JSON string to the file followed by a newline  \n",
    "            file1.write(json_str + '\\n')  \n",
    "            \n",
    "    # Save to line_dict_list into a jsonl file (test)\n",
    "    with open(annotations_jsonl_path_test, 'w') as file2:  \n",
    "        for dictionary2 in line_dict_list_test:  \n",
    "            # Convert the dictionary to a JSON string  \n",
    "            json_str = json.dumps(dictionary2)  \n",
    "            # Write the JSON string to the file followed by a newline  \n",
    "            file2.write(json_str + '\\n') \n",
    "            \n",
    "    print(f'Files created: \\n{annotations_jsonl_path_train}, \\n{annotations_jsonl_path_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55082f6-ecfd-4d12-b65a-bbb52710fdb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_example(model, task_prompt, text_input=None):   \n",
    "    \n",
    "    if text_input is None:\n",
    "        prompt = task_prompt\n",
    "    else:\n",
    "        prompt = task_prompt + text_input\n",
    "    \n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # MK\n",
    "    # Move the Input Data to GPU\n",
    "    if device == 'cuda':\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}  \n",
    "         \n",
    "    generated_ids = model.generate(\n",
    "      input_ids=inputs[\"input_ids\"],\n",
    "      pixel_values=inputs[\"pixel_values\"],\n",
    "      max_new_tokens=1024,\n",
    "      early_stopping=False,\n",
    "      do_sample=False,\n",
    "      num_beams=3,\n",
    "    )\n",
    "    \n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    \n",
    "    parsed_answer = processor.post_process_generation(\n",
    "        generated_text, \n",
    "        task=task_prompt, \n",
    "        image_size=(image.width, image.height)\n",
    "    )\n",
    "\n",
    "    return parsed_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba201c0a-6e59-4414-90c9-06f95ef119f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MK\n",
    "from PIL import Image  \n",
    "import matplotlib.pyplot as plt  \n",
    "import matplotlib.patches as patches  \n",
    "import re  \n",
    "  \n",
    "def plot_normalized_bbox(image, bbox_data):  \n",
    "    # Create a figure and axes  \n",
    "    fig, ax = plt.subplots()  \n",
    "      \n",
    "    # Display the image  \n",
    "    ax.imshow(image)  \n",
    "      \n",
    "    # Get image dimensions  \n",
    "    img_width, img_height = image.size  \n",
    "      \n",
    "    # Parse the normalized bounding box coordinates  \n",
    "    bboxes = re.findall(r\"<loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)>\", bbox_data)  \n",
    "      \n",
    "    # Convert normalized coordinates to absolute coordinates and plot the rectangles  \n",
    "    for bbox in bboxes:  \n",
    "        # Normalize coordinates and convert to float  \n",
    "        x1, y1, x2, y2 = [float(coord)/1000 for coord in bbox]  \n",
    "          \n",
    "        # Convert to absolute coordinates  \n",
    "        abs_x1, abs_y1 = x1 * img_width, y1 * img_height  \n",
    "        abs_x2, abs_y2 = x2 * img_width, y2 * img_height  \n",
    "          \n",
    "        # Create a Rectangle patch  \n",
    "        rect = patches.Rectangle((abs_x1, abs_y1), abs_x2 - abs_x1, abs_y2 - abs_y1, linewidth=1, edgecolor='r', facecolor='none')  \n",
    "          \n",
    "        # Add the rectangle to the Axes  \n",
    "        ax.add_patch(rect)  \n",
    "      \n",
    "    # Remove the axis ticks and labels  \n",
    "    ax.axis('off')  \n",
    "      \n",
    "    # Show the plot  \n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddde049-063b-4abe-8205-b269805654bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_loss(avg_train_loss_list, avg_val_loss_list, size=(10, 5), title='Training vs Validation Loss',   \n",
    "              x_label='Epochs', y_label='Loss', train_legend='Training Loss', val_legend='Validation Loss'):  \n",
    "    \n",
    "    # Set the size of the plot  \n",
    "    plt.figure(figsize=size)  \n",
    "      \n",
    "    # Plot training and validation loss  \n",
    "    plt.plot(avg_train_loss_list, label=train_legend)  \n",
    "    plt.plot(avg_val_loss_list, label=val_legend)  \n",
    "      \n",
    "    # Adding title and labels  \n",
    "    plt.title(title)  \n",
    "    plt.xlabel(x_label)  \n",
    "    plt.ylabel(y_label)  \n",
    "      \n",
    "    # Show legend  \n",
    "    plt.legend()  \n",
    "      \n",
    "    # Show the plot  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aa33d1-29aa-4b52-aaf6-57633ecca32c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Temp. Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3443d1-c6f9-4b61-86cb-fe1a65732615",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Check json loading\n",
    "# annotations_json =load_json(annotations_coco_path)\n",
    "# print(f'annotations_json:{json.dumps(annotations_json, indent=4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f24ba0-1951-4b62-9009-21481b38e0d1",
   "metadata": {},
   "source": [
    "#### Convert to JSONL\n",
    "Annotations from COCO to JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cefe4f0-7aa4-46d0-b020-09bb48877c00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert from coco to jsonl\n",
    "coco_to_jsonl_reg2desc(annotations_coco_path, annotations_jsonl_path_train, annotations_jsonl_path_test, test_size = test_size, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2caed5e-8538-4a49-8bb7-589fff6fac79",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ee6c2-b511-46c7-9349-2c6fa599f1f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "processor = None\n",
    "\n",
    "if revision:\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).eval().to(device)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, revision = revision).to(device)\n",
    "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True, revision = revision)\n",
    "else: # Ignore revision\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(device)\n",
    "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)  \n",
    "\n",
    "print(f'Model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addba7b1-ef07-4860-b1d7-716fd7b58ed2",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd0ed4-a41f-4044-838e-b4e2e92a529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Dataset and DataLoader for train and validation subsets\n",
    "\n",
    "def collate_fn(batch):\n",
    "    questions, answers, images = zip(*batch)\n",
    "    # print(f'questions: {questions}, answers: {answers}, images:{images}')\n",
    "    inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True).to(device)\n",
    "    return inputs, answers\n",
    "\n",
    "train_dataset = Region2DescDataset(\n",
    "    jsonl_file_path = annotations_jsonl_path_train,\n",
    "    image_directory_path = images_base_path\n",
    ")\n",
    "\n",
    "val_dataset = Region2DescDataset(\n",
    "    jsonl_file_path = annotations_jsonl_path_test,\n",
    "    image_directory_path = images_base_path\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2eb35-7193-4b86-9924-38fd87789e42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LoRA Florence-2 model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16, #8\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    "    use_rslora=True,\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    revision=revision\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a4171a-42ca-4d3a-adae-8af46421b9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219bd438-dd7b-4267-be88-9033e0f41f99",
   "metadata": {},
   "source": [
    "### Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58a847a-4198-4b86-a3cb-f25e81f1c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):\n",
    "    avg_train_loss_list = []\n",
    "    avg_val_loss_list = []\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    num_training_steps = epochs * len(train_loader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    # render_inference_results(peft_model, val_loader.dataset, 6)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, answers in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
    "\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            pixel_values = inputs[\"pixel_values\"]\n",
    "            labels = processor.tokenizer(\n",
    "                text=answers,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                return_token_type_ids=False\n",
    "            ).input_ids.to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward(), optimizer.step(), lr_scheduler.step(), optimizer.zero_grad()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Average Training Loss: {avg_train_loss}\")\n",
    "        avg_train_loss_list.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, answers in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n",
    "\n",
    "                input_ids = inputs[\"input_ids\"]\n",
    "                pixel_values = inputs[\"pixel_values\"]\n",
    "                labels = processor.tokenizer(\n",
    "                    text=answers,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    return_token_type_ids=False\n",
    "                ).input_ids.to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            print(f\"Average Validation Loss: {avg_val_loss}\")\n",
    "            avg_val_loss_list.append(avg_val_loss)\n",
    "\n",
    "            # render_inference_results(peft_model, val_loader.dataset, 6)\n",
    "\n",
    "    # MK\n",
    "    # Save last epoch\n",
    "    output_dir = f\"../model_checkpoints/epoch_{epoch+1}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save_pretrained(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    \n",
    "    return avg_train_loss_list, avg_val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdcc0e3-24b5-44ee-a104-e3c900bd09c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "avg_train_loss_list, avg_val_loss_list = train_model(train_loader, val_loader, peft_model, processor, epochs=EPOCHS, lr=LR)\n",
    "\n",
    "plot_loss(avg_train_loss_list, avg_val_loss_list, size=(6, 3), title='Training vs Validation Loss',   \n",
    "              x_label='Epochs', y_label='Loss', train_legend='Training Loss', val_legend='Validation Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd5ecda-c40f-41c1-98c6-09663131d1f5",
   "metadata": {},
   "source": [
    "### Load fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3746833-e15d-474c-a9ad-61c1659b57f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checkpoint_path = '../model_checkpoints/' + model_id.replace('/','_').replace('-','_').replace(' ','_') + '_epoch_' + str(EPOCHS)\n",
    "checkpoint_path = '../model_checkpoints/' + 'epoch_' + str(EPOCHS)\n",
    "\n",
    "model_ft = AutoModelForCausalLM.from_pretrained(checkpoint_path, trust_remote_code=True, revision = revision).to(device)\n",
    "processor = AutoProcessor.from_pretrained(checkpoint_path, trust_remote_code=True, revision = revision)\n",
    "\n",
    "print(f'Loaded checkpoint_path:{checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37a6f08-a43e-42ea-9871-0b256bd58e74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_own_image = True\n",
    "# image_path = '../test_images/maksssksksss0.png'\n",
    "image_path = os.path.join(images_base_path,'maksssksksss19.png')\n",
    "\n",
    "print(f'use_own_image: {use_own_image}')\n",
    "\n",
    "if not use_own_image:\n",
    "    url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "else:    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "# See input image\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab3740-bc34-40b1-960c-15ae3d3766b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This sample taken from jsonl file\n",
    "'''{\"image\": \"maksssksksss7.png\", \"prefix\": \"<REGION_TO_DESCRIPTION>\", \"suffix\": \"\n",
    "Wearing a mask<loc_614><loc_232><loc_882><loc_706>\n",
    "Wearing a mask<loc_362><loc_197><loc_573><loc_588>\n",
    "Wearing a mask<loc_221><loc_382><loc_432><loc_645>\n",
    "Not wearing a mask<loc_275><loc_161><loc_350><loc_307>\"}\n",
    "'''\n",
    "\n",
    "'''\n",
    "{\"image\": \"maksssksksss2.png\", \"prefix\": \"<REGION_TO_DESCRIPTION>\", \"suffix\": \"\n",
    "Wearing a mask<loc_591><loc_52><loc_707><loc_238>\n",
    "Wearing a mask<loc_824><loc_15><loc_954><loc_234>\n",
    "Wearing a mask<loc_366><loc_82><loc_473><loc_272>\n",
    "Wearing a mask<loc_161><loc_49><loc_275><loc_247>\"}\n",
    "'''\n",
    "\n",
    "'''{\"image\": \"maksssksksss19.png\", \"prefix\": \"<REGION_TO_DESCRIPTION>\", \"suffix\": \"\n",
    "Wearing a mask<loc_215><loc_90><loc_296><loc_263>\n",
    "Wearing a mask<loc_745><loc_56><loc_979><loc_582>\n",
    "Wearing a mask<loc_238><loc_180><loc_499><loc_739>\n",
    "Not wearing a mask<loc_662><loc_17><loc_716><loc_136>\n",
    "Not wearing a mask<loc_0><loc_141><loc_53><loc_287>\"}'''\n",
    "\n",
    "# MK\n",
    "# Added to adjust the region coordinates as per your preference in the given image\n",
    "# custom_region = \"<loc_320><loc_200><loc_450><loc_400>\"\n",
    "custom_region = \"<loc_0><loc_141><loc_53><loc_287>\"\n",
    "plot_normalized_bbox(image, custom_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c99c72-debc-44d6-98d4-c39ca0fbda75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "task_prompt = '<REGION_TO_DESCRIPTION>'\n",
    "print(f'use_own_image:{use_own_image}')\n",
    "\n",
    "if not use_own_image:    \n",
    "    results = run_example(model_ft,task_prompt, text_input=\"<loc_52><loc_332><loc_932><loc_774>\")\n",
    "else:\n",
    "    results = run_example(model_ft,task_prompt, text_input=custom_region)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (huggingface1)",
   "language": "python",
   "name": "huggingface1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
