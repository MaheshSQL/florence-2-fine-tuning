{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "344bd596-3091-4b2c-b9b6-19e5f3a6b2ce",
   "metadata": {},
   "source": [
    "Kernel: huggingface1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb2c2b-4018-45e1-9046-94c9aae7ac96",
   "metadata": {},
   "source": [
    "Ref: https://huggingface.co/microsoft/Florence-2-large/blob/main/sample_inference.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10c7fa-7a8a-4592-a704-27908d049247",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c40ad2-230e-4a96-a69d-2c02f7c99143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828a5530-713e-4fb9-b56d-6c78a6882472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if CUDA is available  \n",
    "cuda_available = torch.cuda.is_available()  \n",
    "  \n",
    "print(\"Is CUDA available? \", cuda_available)  \n",
    "\n",
    "# Print CUDA version  \n",
    "print(\"CUDA version:\", torch.version.cuda)  \n",
    "  \n",
    "# If CUDA is available, print the number of GPUs and their names  \n",
    "if cuda_available:  \n",
    "    print(\"Number of GPUs available: \", torch.cuda.device_count())  \n",
    "    for i in range(torch.cuda.device_count()):  \n",
    "        print(\"GPU \", i, \": \", torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51fcb7-735c-4412-ad7d-53b2982312ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MK\n",
    "device = 'cpu'\n",
    "\n",
    "if cuda_available:\n",
    "    device='cuda'\n",
    "    print(f'device:{device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab4828f-bdb5-4449-a527-f16d3ea850b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM  \n",
    "from PIL import Image\n",
    "import requests\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f80d4-092c-494b-96cc-481285e3412d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_id = 'microsoft/Florence-2-base'\n",
    "# model_id = 'microsoft/Florence-2-large'\n",
    "model_id = 'microsoft/Florence-2-base-ft'\n",
    "# model_id = 'microsoft/Florence-2-large-ft'\n",
    "print(f'model_id:{model_id}')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).eval()\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa5111-de31-4535-8f34-971b84ae4b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915381fb-2ecb-43f5-a0a7-e675d9fc5cc8",
   "metadata": {},
   "source": [
    "#### The prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a348a95-48e1-455d-a236-1eb7ad521b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_example(task_prompt, text_input=None):   \n",
    "    \n",
    "    if text_input is None:\n",
    "        prompt = task_prompt\n",
    "    else:\n",
    "        prompt = task_prompt + text_input\n",
    "    \n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # MK\n",
    "    # Move the Input Data to GPU\n",
    "    if device == 'cuda':\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}  \n",
    "         \n",
    "    generated_ids = model.generate(\n",
    "      input_ids=inputs[\"input_ids\"],\n",
    "      pixel_values=inputs[\"pixel_values\"],\n",
    "      max_new_tokens=1024,\n",
    "      early_stopping=False,\n",
    "      do_sample=False,\n",
    "      num_beams=3,\n",
    "    )\n",
    "    \n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    \n",
    "    parsed_answer = processor.post_process_generation(\n",
    "        generated_text, \n",
    "        task=task_prompt, \n",
    "        image_size=(image.width, image.height)\n",
    "    )\n",
    "\n",
    "    return parsed_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f318bda3-cc87-4cbc-a5a2-8ed264e03100",
   "metadata": {},
   "source": [
    "#### Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9e49ab-1de6-44d1-ba79-5f775b6a91b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "import matplotlib.patches as patches  \n",
    "\n",
    "def plot_bbox(image, data):\n",
    "   # Create a figure and axes  \n",
    "    fig, ax = plt.subplots()  \n",
    "      \n",
    "    # Display the image  \n",
    "    ax.imshow(image)  \n",
    "      \n",
    "    # Plot each bounding box  \n",
    "    for bbox, label in zip(data['bboxes'], data['labels']):  \n",
    "        # Unpack the bounding box coordinates  \n",
    "        x1, y1, x2, y2 = bbox  \n",
    "        # Create a Rectangle patch  \n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')  \n",
    "        # Add the rectangle to the Axes  \n",
    "        ax.add_patch(rect)  \n",
    "        # Annotate the label  \n",
    "        plt.text(x1, y1, label, color='white', fontsize=8, bbox=dict(facecolor='red', alpha=0.5))  \n",
    "      \n",
    "    # Remove the axis ticks and labels  \n",
    "    ax.axis('off')  \n",
    "      \n",
    "    # Show the plot  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0b59b2-6654-4408-95f7-0c107c5e65de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MK\n",
    "from PIL import Image  \n",
    "import matplotlib.pyplot as plt  \n",
    "import matplotlib.patches as patches  \n",
    "import re  \n",
    "  \n",
    "def plot_normalized_bbox(image, bbox_data):  \n",
    "    # Create a figure and axes  \n",
    "    fig, ax = plt.subplots()  \n",
    "      \n",
    "    # Display the image  \n",
    "    ax.imshow(image)  \n",
    "      \n",
    "    # Get image dimensions  \n",
    "    img_width, img_height = image.size  \n",
    "      \n",
    "    # Parse the normalized bounding box coordinates  \n",
    "    bboxes = re.findall(r\"<loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)>\", bbox_data)  \n",
    "      \n",
    "    # Convert normalized coordinates to absolute coordinates and plot the rectangles  \n",
    "    for bbox in bboxes:  \n",
    "        # Normalize coordinates and convert to float  \n",
    "        x1, y1, x2, y2 = [float(coord)/1000 for coord in bbox]  \n",
    "          \n",
    "        # Convert to absolute coordinates  \n",
    "        abs_x1, abs_y1 = x1 * img_width, y1 * img_height  \n",
    "        abs_x2, abs_y2 = x2 * img_width, y2 * img_height  \n",
    "          \n",
    "        # Create a Rectangle patch  \n",
    "        rect = patches.Rectangle((abs_x1, abs_y1), abs_x2 - abs_x1, abs_y2 - abs_y1, linewidth=1, edgecolor='r', facecolor='none')  \n",
    "          \n",
    "        # Add the rectangle to the Axes  \n",
    "        ax.add_patch(rect)  \n",
    "      \n",
    "    # Remove the axis ticks and labels  \n",
    "    ax.axis('off')  \n",
    "      \n",
    "    # Show the plot  \n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6322d794-b720-4252-af1c-fd5a83f65241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "colormap = ['blue','orange','green','purple','brown','pink','gray','olive','cyan','red',\n",
    "            'lime','indigo','violet','aqua','magenta','coral','gold','tan','skyblue']\n",
    "def draw_polygons(image, prediction, fill_mask=False):  \n",
    "    \"\"\"  \n",
    "    Draws segmentation masks with polygons on an image.  \n",
    "  \n",
    "    Parameters:  \n",
    "    - image_path: Path to the image file.  \n",
    "    - prediction: Dictionary containing 'polygons' and 'labels' keys.  \n",
    "                  'polygons' is a list of lists, each containing vertices of a polygon.  \n",
    "                  'labels' is a list of labels corresponding to each polygon.  \n",
    "    - fill_mask: Boolean indicating whether to fill the polygons with color.  \n",
    "    \"\"\"  \n",
    "    # Load the image  \n",
    "   \n",
    "    draw = ImageDraw.Draw(image)  \n",
    "      \n",
    "   \n",
    "    # Set up scale factor if needed (use 1 if not scaling)  \n",
    "    scale = 1  \n",
    "      \n",
    "    # Iterate over polygons and labels  \n",
    "    for polygons, label in zip(prediction['polygons'], prediction['labels']):  \n",
    "        color = random.choice(colormap)  \n",
    "        fill_color = random.choice(colormap) if fill_mask else None  \n",
    "          \n",
    "        for _polygon in polygons:  \n",
    "            _polygon = np.array(_polygon).reshape(-1, 2)  \n",
    "            if len(_polygon) < 3:  \n",
    "                print('Invalid polygon:', _polygon)  \n",
    "                continue  \n",
    "              \n",
    "            _polygon = (_polygon * scale).reshape(-1).tolist()  \n",
    "              \n",
    "            # Draw the polygon  \n",
    "            if fill_mask:  \n",
    "                draw.polygon(_polygon, outline=color, fill=fill_color)  \n",
    "            else:  \n",
    "                draw.polygon(_polygon, outline=color)  \n",
    "              \n",
    "            # Draw the label text  \n",
    "            draw.text((_polygon[0] + 8, _polygon[1] + 2), label, fill=color)  \n",
    "  \n",
    "    # Save or display the image  \n",
    "    #image.show()  # Display the image  \n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812470b-5e17-4b3c-9240-3c930d0e8811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_od_format(data):  \n",
    "    \"\"\"  \n",
    "    Converts a dictionary with 'bboxes' and 'bboxes_labels' into a dictionary with separate 'bboxes' and 'labels' keys.  \n",
    "  \n",
    "    Parameters:  \n",
    "    - data: The input dictionary with 'bboxes', 'bboxes_labels', 'polygons', and 'polygons_labels' keys.  \n",
    "  \n",
    "    Returns:  \n",
    "    - A dictionary with 'bboxes' and 'labels' keys formatted for object detection results.  \n",
    "    \"\"\"  \n",
    "    # Extract bounding boxes and labels  \n",
    "    bboxes = data.get('bboxes', [])  \n",
    "    labels = data.get('bboxes_labels', [])  \n",
    "      \n",
    "    # Construct the output format  \n",
    "    od_results = {  \n",
    "        'bboxes': bboxes,  \n",
    "        'labels': labels  \n",
    "    }  \n",
    "      \n",
    "    return od_results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf57556-3f2f-4dd9-b122-d05d59a2efdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_ocr_bboxes(image, prediction):\n",
    "    scale = 1\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    bboxes, labels = prediction['quad_boxes'], prediction['labels']\n",
    "    for box, label in zip(bboxes, labels):\n",
    "        color = random.choice(colormap)\n",
    "        new_box = (np.array(box) * scale).tolist()\n",
    "        draw.polygon(new_box, width=3, outline=color)\n",
    "        draw.text((new_box[0]+8, new_box[1]+2),\n",
    "                    \"{}\".format(label),\n",
    "                    align=\"right\",\n",
    "        \n",
    "                    fill=color)\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d892c9-3856-4e9a-bd9d-68959b38d79e",
   "metadata": {},
   "source": [
    "#### Initialise the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113138ba-d40b-4e2d-abcc-1eac65c3d062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from PIL import Image, ImageFile  \n",
    "  \n",
    "# # Allow loading of truncated images  \n",
    "# ImageFile.LOAD_TRUNCATED_IMAGES = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3236b241-81a0-4ff7-a5c4-d18d75f337d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_own_image = True\n",
    "image_path = '../test_images/maksssksksss0.png'\n",
    "# image_path = '../test_images/test29.png'\n",
    "print(f'use_own_image: {use_own_image}')\n",
    "\n",
    "if not use_own_image:\n",
    "    url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "else:    \n",
    "    image = Image.open(image_path).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f178c8-57fe-46fa-995d-a3493f3eac83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f82c6ff-0c6d-4d35-a890-c0852d804c33",
   "metadata": {},
   "source": [
    "#### Run pre-defined tasks without additional inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d7a5ee-9ea6-4b25-8b58-b063e5dd667a",
   "metadata": {},
   "source": [
    "Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b856830a-2e71-4a2d-be57-72202f7e6db7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "task_prompt = '<CAPTION>'\n",
    "run_example(task_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c5a0e6-7a24-4eb7-b928-0dacff9a38dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "task_prompt = '<DETAILED_CAPTION>'\n",
    "run_example(task_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55a675e-ad68-4eb8-81fe-ba827577a762",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# task_prompt = 'what is person the right doing'\n",
    "# task_prompt = 'how many people are wearing warm clothes'\n",
    "# task_prompt = 'what are people in the background doing'\n",
    "# task_prompt = 'how many people can be seen in the background'\n",
    "# task_prompt = 'describe what is this place shown in the image'\n",
    "\n",
    "q_list = ['What are people doing?',\n",
    "          'Does this look like a photo taken indoors?',\n",
    "          'Is this photo taken during the day or night?',\n",
    "          'Are people carrying any items?']\n",
    "\n",
    "for task_prompt in q_list:  \n",
    "\n",
    "    results = run_example(task_prompt)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee714ae-b029-4349-8c5f-a685ac9fdb7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "task_prompt = '<MORE_DETAILED_CAPTION>'\n",
    "run_example(task_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf8e1e-468f-4d5e-af6f-82781ed4b1ab",
   "metadata": {},
   "source": [
    "Object Detection\n",
    "\n",
    "OD results format: {'<OD>': { 'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['label1', 'label2', ...] } }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd2fe77-fb01-4b64-9b1c-5b9535a5ed1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "task_prompt = '<OD>'\n",
    "results = run_example(task_prompt)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa78c9-223a-4685-adb8-397abdd4fc56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_bbox(image, results['<OD>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ffbaa-6153-4558-8617-91cf671114e8",
   "metadata": {},
   "source": [
    "Dense region caption\n",
    "\n",
    "Dense region caption results format: {'<DENSE_REGION_CAPTION>': {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['label1', 'label2', ...]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041aafb4-70f7-410e-a141-53e8f6c26acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "task_prompt = '<DENSE_REGION_CAPTION>'\n",
    "results = run_example(task_prompt)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd62e279-3949-44cd-809b-37cdf297e88c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_bbox(image, results['<DENSE_REGION_CAPTION>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb40680-5684-4628-98d4-b6c3e5994eb9",
   "metadata": {},
   "source": [
    "Region proposal\n",
    "\n",
    "Region proposal results format: {'' : {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['', '', ...]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d440ba1e-a283-4123-ba31-50cda9a2c5c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "task_prompt = '<REGION_PROPOSAL>'\n",
    "results = run_example(task_prompt)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333c052-fc22-40db-b6d3-cbce8f399c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_bbox(image, results['<REGION_PROPOSAL>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa82c4f-c5d9-471b-bedd-290c71021367",
   "metadata": {},
   "source": [
    "#### Run pre-defined tasks that requires additional inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d1bb43-14a8-4294-ae78-93a6995830cc",
   "metadata": {},
   "source": [
    "Phrase Grounding\n",
    "\n",
    "Phrase grounding results format: {'<CAPTION_TO_PHRASE_GROUNDING>': {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['', '', ...]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacbddf5-e266-40a2-8b0f-beda07700729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "task_prompt = '<CAPTION_TO_PHRASE_GROUNDING>'\n",
    "results = run_example(task_prompt, text_input=\"A green car parked in front of a yellow building.\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789fb6d-6719-4b03-8f7e-b63d023ad871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_bbox(image, results['<CAPTION_TO_PHRASE_GROUNDING>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf544c-c0af-4984-9b43-976d94f69588",
   "metadata": {},
   "source": [
    "Referring expression segmentation\n",
    "\n",
    "Referring expression segmentation results format: {'<REFERRING_EXPRESSION_SEGMENTATION>': {'Polygons': [[[polygon]], ...], 'labels': ['', '', ...]}}, one object is represented by a list of polygons. each polygon is [x1, y1, x2, y2, ..., xn, yn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5354c01e-a4d1-4f88-a078-dbd0cbddfe45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "task_prompt = '<REFERRING_EXPRESSION_SEGMENTATION>'\n",
    "results = run_example(task_prompt, text_input=\"a green car\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578bd3b-0d4b-4bbc-b448-2b3c7b1dd2c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_image = copy.deepcopy(image)\n",
    "draw_polygons(output_image, results['<REFERRING_EXPRESSION_SEGMENTATION>'], fill_mask=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09d4c03-eb9b-40bb-90d9-a56295c653ee",
   "metadata": {},
   "source": [
    "region to segmentation\n",
    "\n",
    "with additional region as inputs, format is '<loc_x1><loc_y1><loc_x2><loc_y2>', [x1, y1, x2, y2] is the quantized corrdinates in [0, 999]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8892a23-3c03-4a3b-b962-d032a49d39af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "task_prompt = '<REGION_TO_SEGMENTATION>'\n",
    "results = run_example(task_prompt, text_input=\"<loc_702><loc_575><loc_866><loc_772>\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282040f8-701f-4234-9a14-55827c319fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_image = copy.deepcopy(image)\n",
    "draw_polygons(output_image, results['<REGION_TO_SEGMENTATION>'], fill_mask=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47c4c5-ba7e-49b3-b29f-06d3e5373878",
   "metadata": {},
   "source": [
    "Open vocabulary detection\n",
    "\n",
    "open vocabulary detection can detect both objects and ocr texts.\n",
    "\n",
    "results format:\n",
    "\n",
    "{ '<OPEN_VOCABULARY_DETECTION>': {'bboxes': [[x1, y1, x2, y2], [x1, y1, x2, y2], ...]], 'bboxes_labels': ['label_1', 'label_2', ..], 'polygons': [[[x1, y1, x2, y2, ..., xn, yn], [x1, y1, ..., xn, yn]], ...], 'polygons_labels': ['label_1', 'label_2', ...] }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc94c570-0abd-4bd8-a9ca-2744d357050c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "task_prompt = '<OPEN_VOCABULARY_DETECTION>'\n",
    "# results = run_example(task_prompt, text_input=\"a green car\")\n",
    "results = run_example(task_prompt, text_input=\"yellow wall\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c5a863-7d77-46f9-b96e-e26878d711b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bbox_results  = convert_to_od_format(results['<OPEN_VOCABULARY_DETECTION>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76bbae1-1a83-480c-be5f-4c03ffe71298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_bbox(image, bbox_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3855fb3-b25d-4421-ad52-8d9a668a3133",
   "metadata": {},
   "source": [
    "region to texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1158081-e184-44db-b834-02e399b08bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MK\n",
    "# Added to adjust the region coordinates as per your preference in the given image\n",
    "custom_region = \"<loc_320><loc_200><loc_450><loc_400>\"\n",
    "plot_normalized_bbox(image, custom_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83321ba-8990-4fb6-a13a-c6226daba660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "task_prompt = '<REGION_TO_CATEGORY>'\n",
    "if not use_own_image:\n",
    "    results = run_example(task_prompt, text_input=\"<loc_52><loc_332><loc_932><loc_774>\")\n",
    "else:\n",
    "    results = run_example(task_prompt, text_input=custom_region)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd66b57f-c26f-43a0-b1d5-2f3ee4e73ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "task_prompt = '<REGION_TO_DESCRIPTION>'\n",
    "if not use_own_image:\n",
    "    results = run_example(task_prompt, text_input=\"<loc_52><loc_332><loc_932><loc_774>\")\n",
    "else:\n",
    "    results = run_example(task_prompt, text_input=custom_region)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec763dd4-4b7c-4b37-aa03-f4c1d5a9a720",
   "metadata": {},
   "source": [
    "ocr related tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e52d0a-02be-44a6-b00e-a62870e7663a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"http://ecx.images-amazon.com/images/I/51UUzBDAMsL.jpg?download=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2350b7e4-88de-4944-88d6-0452ee83057e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0405c48e-d5a5-4ec6-8cdb-d328239abdf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "task_prompt = '<OCR>'\n",
    "run_example(task_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a3f5a-9428-4299-8f1d-336d628947af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "task_prompt = '<OCR_WITH_REGION>'\n",
    "results = run_example(task_prompt)\n",
    "print(results)\n",
    "# ocr results format\n",
    "# {'OCR_WITH_REGION': {'quad_boxes': [[x1, y1, x2, y2, x3, y3, x4, y4], ...], 'labels': ['text1', ...]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feea70a-7f98-41c6-86c0-72f33ccd25ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_image = copy.deepcopy(image)\n",
    "draw_ocr_bboxes(output_image, results['<OCR_WITH_REGION>'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a06ed05-fdd0-4ad4-8540-2fd62ddfbbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b968c9f-5ade-408c-839b-58f30af5e035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (huggingface1)",
   "language": "python",
   "name": "huggingface1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
